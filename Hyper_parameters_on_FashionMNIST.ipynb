{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOG5lAuM6BDZKP00PH/32/b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahan2210/pytorch_practice/blob/main/Hyper_parameters_on_FashionMNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GefL-IPiXKgJ",
        "outputId": "9f13c86c-c9be-4428-8f11-59afc0414a5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:08<00:00, 3129872.80it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 276913.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 2731697.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5347270.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "YDWRDMk9YVrp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "qmkIxCcLYlBv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: define ADAM\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "dLdohTOMYsEG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= size\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "n9fIAAhWY7Ix"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNhg4JsiaClm",
        "outputId": "85b841bd-94b8-4236-c0f5-1d94d0ac5d76"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.308246  [    0/60000]\n",
            "loss: 1.640470  [ 6400/60000]\n",
            "loss: 1.602547  [12800/60000]\n",
            "loss: 1.698169  [19200/60000]\n",
            "loss: 1.715338  [25600/60000]\n",
            "loss: 1.437456  [32000/60000]\n",
            "loss: 1.387707  [38400/60000]\n",
            "loss: 1.497806  [44800/60000]\n",
            "loss: 1.500485  [51200/60000]\n",
            "loss: 1.338387  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 38.8%, Avg loss: 0.024047 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.394154  [    0/60000]\n",
            "loss: 1.484221  [ 6400/60000]\n",
            "loss: 1.525491  [12800/60000]\n",
            "loss: 1.606215  [19200/60000]\n",
            "loss: 1.678607  [25600/60000]\n",
            "loss: 1.388179  [32000/60000]\n",
            "loss: 1.352727  [38400/60000]\n",
            "loss: 1.483843  [44800/60000]\n",
            "loss: 1.440135  [51200/60000]\n",
            "loss: 1.247280  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.0%, Avg loss: 0.023436 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.361875  [    0/60000]\n",
            "loss: 1.456051  [ 6400/60000]\n",
            "loss: 1.479253  [12800/60000]\n",
            "loss: 1.526226  [19200/60000]\n",
            "loss: 1.651388  [25600/60000]\n",
            "loss: 1.359647  [32000/60000]\n",
            "loss: 1.336265  [38400/60000]\n",
            "loss: 1.446654  [44800/60000]\n",
            "loss: 1.446447  [51200/60000]\n",
            "loss: 1.203148  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.3%, Avg loss: 0.023252 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.327363  [    0/60000]\n",
            "loss: 1.424306  [ 6400/60000]\n",
            "loss: 1.471516  [12800/60000]\n",
            "loss: 1.521154  [19200/60000]\n",
            "loss: 1.639719  [25600/60000]\n",
            "loss: 1.314094  [32000/60000]\n",
            "loss: 1.300903  [38400/60000]\n",
            "loss: 1.438570  [44800/60000]\n",
            "loss: 1.397354  [51200/60000]\n",
            "loss: 1.178197  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.1%, Avg loss: 0.023115 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.321732  [    0/60000]\n",
            "loss: 1.389517  [ 6400/60000]\n",
            "loss: 1.456080  [12800/60000]\n",
            "loss: 1.499222  [19200/60000]\n",
            "loss: 1.648863  [25600/60000]\n",
            "loss: 1.323336  [32000/60000]\n",
            "loss: 1.271579  [38400/60000]\n",
            "loss: 1.441015  [44800/60000]\n",
            "loss: 1.361206  [51200/60000]\n",
            "loss: 1.183038  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.6%, Avg loss: 0.022920 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.323498  [    0/60000]\n",
            "loss: 1.396628  [ 6400/60000]\n",
            "loss: 1.449375  [12800/60000]\n",
            "loss: 1.491474  [19200/60000]\n",
            "loss: 1.606314  [25600/60000]\n",
            "loss: 1.317552  [32000/60000]\n",
            "loss: 1.257802  [38400/60000]\n",
            "loss: 1.405314  [44800/60000]\n",
            "loss: 1.330988  [51200/60000]\n",
            "loss: 1.159917  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.1%, Avg loss: 0.022999 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.330524  [    0/60000]\n",
            "loss: 1.392918  [ 6400/60000]\n",
            "loss: 1.458187  [12800/60000]\n",
            "loss: 1.467663  [19200/60000]\n",
            "loss: 1.587752  [25600/60000]\n",
            "loss: 1.296827  [32000/60000]\n",
            "loss: 1.249846  [38400/60000]\n",
            "loss: 1.350482  [44800/60000]\n",
            "loss: 1.304731  [51200/60000]\n",
            "loss: 1.149257  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 41.1%, Avg loss: 0.022697 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.296107  [    0/60000]\n",
            "loss: 1.380247  [ 6400/60000]\n",
            "loss: 1.452493  [12800/60000]\n",
            "loss: 1.488324  [19200/60000]\n",
            "loss: 1.572885  [25600/60000]\n",
            "loss: 1.288296  [32000/60000]\n",
            "loss: 1.253176  [38400/60000]\n",
            "loss: 1.342802  [44800/60000]\n",
            "loss: 1.292428  [51200/60000]\n",
            "loss: 1.126147  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.3%, Avg loss: 0.023027 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.332481  [    0/60000]\n",
            "loss: 1.372089  [ 6400/60000]\n",
            "loss: 1.447977  [12800/60000]\n",
            "loss: 1.470438  [19200/60000]\n",
            "loss: 1.568813  [25600/60000]\n",
            "loss: 1.261175  [32000/60000]\n",
            "loss: 1.291685  [38400/60000]\n",
            "loss: 1.356037  [44800/60000]\n",
            "loss: 1.289882  [51200/60000]\n",
            "loss: 1.120760  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.2%, Avg loss: 0.023103 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 1.277813  [    0/60000]\n",
            "loss: 1.361397  [ 6400/60000]\n",
            "loss: 1.450298  [12800/60000]\n",
            "loss: 1.503882  [19200/60000]\n",
            "loss: 1.560207  [25600/60000]\n",
            "loss: 1.298201  [32000/60000]\n",
            "loss: 1.268081  [38400/60000]\n",
            "loss: 1.375538  [44800/60000]\n",
            "loss: 1.310403  [51200/60000]\n",
            "loss: 1.142759  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.9%, Avg loss: 0.022994 \n",
            "\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"data/Hyper-parameters on FashionMNIST.pth\")\n",
        "\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMpPydJWbGuT",
        "outputId": "89734085-1403-4c50-ee58-65faa98de803"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ]
    }
  ]
}